{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Pakete installieren und OpenAI-API-Key laden"
      ],
      "metadata": {
        "id": "qH7P21m1JzIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pakete installieren\n",
        "\n",
        "# Schnittstelle zu OpenAI-Modellen\n",
        "!pip -q install langchain_openai\n",
        "\n",
        "# Zusätzliche LangChain-Komponenten (Loader, Vectorstores)\n",
        "!pip -q install langchain-community\n",
        "\n",
        "# PDF-Parser\n",
        "!pip -q install pypdf\n",
        "\n",
        "# In-Memory-Vektorstore\n",
        "!pip -q install docarray\n",
        "\n",
        "# Tokeenizer für OpenAI-Modelle\n",
        "!pip -q install tiktoken\n",
        "\n",
        "# OpenAI-API-Key laden\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('apikey_ab')\n",
        "\n",
        "# Bricht ab, falls kein API-Key vorhanden ist\n",
        "assert OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "-VEng8kkIcUi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) PDF aus GitHub laden\n"
      ],
      "metadata": {
        "id": "a3Nzoz72KrQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliothek für HTTP-Anfragen (Dateien, APIs etc.)\n",
        "import requests\n",
        "\n",
        "# LangChain-Loader für PDFs\n",
        "# Nutzt pypdf, um Seiten auszulesen und in Dokument-Objekte (Text + Metadaten) zu konvertieren\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# GitHub Raw-Link zur PDF\n",
        "url = \"https://raw.githubusercontent.com/x8bean/Machine-Learning-Assignment/main/Wissensquelle.pdf\"\n",
        "\n",
        "# Download + Speichern als temporäre Datei in Colab\n",
        "pdf_path = \"/content/tmp.pdf\"\n",
        "\n",
        "# Öffnet die Zieldatei im Schreib-/Binärmodus (\"wb\"), lädt die PDF von der angegebenen URL herunter (mit 30 Sekunden Timeout) und schreibt den Inhalt direkt in diese Datei\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(requests.get(url, timeout=30).content)\n",
        "\n",
        "# PDF in LangChain-Dokumente laden\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "page_docs = loader.load()\n",
        "\n",
        "# Überprüfung, ob die PDF die Vorgabe von ≤ 10 Seiten erfüllt\n",
        "assert len(page_docs) <= 10, f\"PDF hat {len(page_docs)} Seiten (>10).\"\n",
        "\n",
        "# Anzahl der Seiten wird ausgegeben\n",
        "print(\"Seiten geladen:\", len(page_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMjZvsxU_Ei1",
        "outputId": "58c42d71-8e53-46e8-d62d-ed590ccf427d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seiten geladen: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Chunking"
      ],
      "metadata": {
        "id": "ONR_btH4M2_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Teilt Texte rekursiv anhand von Trennzeichen (Absatz, Satz, Wort) in Chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialisiert den Textsplitter mit Regeln für Größe, Überlappung und Trennzeichen-Priorität\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800, # Maximale Zeichen pro Chunk\n",
        "    chunk_overlap=120, # Überlappung zwischen Chunks (verhindert Informationsverlust)\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Wendet das Chunking auf alle Seiten-Dokumente an, erzeugt Liste von kleineren Document-Objekten\n",
        "chunks = splitter.split_documents(page_docs)\n",
        "\n",
        "# Anzahl der erzeugten Chunks wird ausgegeben\n",
        "print(\"Chunks erstellt:\", len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5go7SuLRM8Gt",
        "outputId": "7ce8bb1c-ec39-4dc8-9448-5b4af4d98762"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks erstellt: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Embedding"
      ],
      "metadata": {
        "id": "nE6VkLKiNtm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Schnittstelle zu OpenAI-Embedding-API, um Text in Embeddings zu verwandeln, die den Sinn des Textes darstellen\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# In-Memory-Vektorstore, speichert Embedding-Vektoren und führt Ähnlichkeitssuche durch\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# Erstellt Embedding-Funktion, die bei Aufruf OpenAI-API für Vektorisierung nutzt\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\", # Modellauswahl für die Umwandlung von Text in Embeddings\n",
        "    openai_api_key=OPENAI_API_KEY   # API-Schlüssel für die Authentifizierung bei OpenAI\n",
        ")\n",
        "\n",
        "# Wandelt alle Chunks in Embeddings um und speichert sie im Vektorstore\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(chunks, embedding=embeddings)\n",
        "\n",
        "# Erzeugt Retriever, der für eine Suchanfrage die Top-4 relevantesten Chunks ausgibt\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\", # Maximal Marginal Relevance: bedeutet, dass nicht nur die relevantesten Abschnitte ausgewählt werden, sondern auch möglichst unterschiedliche, um Wiederholungen zu vermeiden\n",
        "    search_kwargs={\"k\": 4, \"fetch_k\": 20} # k=4: gibt die 4 besten Treffer zurück (werden später ins Prompt eingefügt); fetch_k=20: zieht zuerst die 20 besten Kandidaten, bevor daraus die 4 ausgewählt werden\n",
        ")"
      ],
      "metadata": {
        "id": "gfIHtdbrNyHj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Implementierung des RAG-Systems"
      ],
      "metadata": {
        "id": "WqsEXyyvOSg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Schnittstelle zu OpenAI-Chat-LLMs\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Generiert strukturierte Prompts aus Template-Texten mit Platzhaltern\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Wandelt LLM-Ausgabe in reinen Text-String um\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Um mehrere Verarbeitungsschritte miteinander zu verbinden:\n",
        "# RunnableParallel: führt mehrere Aufgaben gleichzeitig aus\n",
        "# RunnablePassthrough: gibt die Eingabe unverändert weiter\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Prompt-Template mit Platzhaltern für Kontext und Frage\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem Kontext.\n",
        "Wenn Du die Frage nicht beantworten kannst, antworte \"Ich weiß es nicht\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Erzeugt Prompt-Objekt, das bei Aufruf Platzhalter durch echte Werte ersetzt\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Initialisiert LLM mit Parametern\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\", # Chat-Modellauswahl\n",
        "    openai_api_key=OPENAI_API_KEY, # API-Schlüssel für die Authentifizierung bei OpenAI\n",
        "    temperature=0.2, # Zufälligkeit der Ausgabe steuern\n",
        "    max_tokens=200 # Antwortlänge begrenzen\n",
        ")\n",
        "\n",
        "# Erstellt Parser, der die reine Textausgabe extrahiert\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Kombiniert Retrieval und Frage in einer parallelen Struktur\n",
        "setup = RunnableParallel(\n",
        "    context=retriever, # Führt semantische Suche aus und liefert Chunks als Kontext\n",
        "    question=RunnablePassthrough() # Leitet die Frage unverändert weiter\n",
        ")\n",
        "\n",
        "# Verknüpft alle Schritte: Retrieval --> Prompt --> LLM --> Ausgabe\n",
        "chain = setup | prompt | llm | parser"
      ],
      "metadata": {
        "id": "YK2l0aJ1OXJX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Beispiel"
      ],
      "metadata": {
        "id": "ZBEcp2VZPUj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielabfrage 1\n",
        "print(chain.invoke(\"Wovon handelt das Dokument?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBlc-5XHQN_I",
        "outputId": "6e0ff6cf-ddf4-4d38-b7d8-977d3bd64abe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Das Dokument handelt von der Zuschaueranalyse und Bewertung der WM-Berichterstattung der Fernsehsender ARD und ZDF während der Fußball-Weltmeisterschaft 2014. Es enthält Informationen zu Zuschauerzahlen, Marktanteilen, sowie Umfrageergebnisse zur Qualität der Übertragungen und Berichterstattung.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielabfrage 2\n",
        "print(chain.invoke(\"Welche Unterschiede gab es bei den Zuschauerzahlen zwischen Männern und Frauen während der WM 2014?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89OAyw4xPgFn",
        "outputId": "dc01779a-2c18-495e-ca06-48d9009f7bdd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bei der WM 2014 lag der Marktanteil der Männer bei 88,4 Prozent, während der Marktanteil der Frauen bei 79,4 Prozent lag. In absoluten Zahlen sahen sich 16,11 Millionen Frauen und 16,37 Millionen Männer das Finale an. Beim Halbfinale Brasilien – Deutschland war das Verhältnis von Männern zu Frauen mit jeweils 50 Prozent ausgeglichen, wobei die Frauen mit 15,70 Millionen minimal vor den Männern mit 15,66 Millionen lagen.\n"
          ]
        }
      ]
    }
  ]
}